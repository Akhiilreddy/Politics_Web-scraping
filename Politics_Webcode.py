# -*- coding: utf-8 -*-
"""submit_file[bp].ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v283A5_G77bcqILK2Xvv8rjipeH5kbT5
"""

#installing dependencies

pip install pyppeteer nest_asyncio
apt-get update
apt install chromium-chromedriver

'''
1. pyppetree, asynco, nest_asynco can be used in combination in a single method to scrap web data.
2. Since we could only fetch data of 1st page only, by using pyppeteer button actions
   to go to next page can be called where we can scrap the data from all the 4 pages. asyncio allows concurrency i.e parallel execution
3. nest_asyncio allows nested event loop which helps to avoid deadlock in execution.
4. Using these we can serialize the execution of user choice
'''

import asyncio                     #Asyncio allows Concurrency
from pyppeteer import launch       #https://www.youtube.com/watch?v=Egp5ktPZgrI
from bs4 import BeautifulSoup
import re
import nest_asyncio                #allow nested event loops [avoids deadlock]
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sqlite3
import statsmodels.api as sm



nest_asyncio.apply()

async def setup_database(db_name):
    conn = sqlite3.connect(db_name)
    cursor = conn.cursor()
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS criminal_cases (
            Sno VARCHAR,
            Candidate VARCHAR,
            Constituency VARCHAR,
            Party VARCHAR,
            Criminal_Cases VARCHAR,
            Education VARCHAR,
            Total_Assets INTEGER,
            Liabilities VARCHAR
        )
    ''')
    conn.commit()
    return conn, cursor

async def close_database(conn):
    conn.commit()
    conn.close()

async def scrape_data(page, cursor, target_records=None):
    content = await page.content()
    soup = BeautifulSoup(content, 'html.parser')
    rows = soup.find_all('tr')[1:]
    records_count = 0
    for row in rows:
        cells = row.find_all('td')
        if len(cells) >= 8:                  #scraping the data from the website
            sno = cells[0].text.strip()
            candidate = cells[1].text.strip()
            constituency = cells[2].text.strip()
            party = cells[3].text.strip()
            criminal_cases = cells[4].text.strip()
            education = cells[5].text.strip()
            total_assets_str = re.sub(r'[^\d]', '', cells[6].text)
            total_assets = int(total_assets_str) if total_assets_str else 0
            liabilities_str = re.sub(r'[^\d]', '', cells[7].text)
            liabilities = int(liabilities_str) if liabilities_str else 0
            cursor.execute("INSERT INTO criminal_cases (Sno, Candidate, Constituency, Party, Criminal_Cases, Education, Total_Assets, Liabilities) VALUES (?, ?, ?, ?, ?, ?, ?, ?)",
                           (sno, candidate, constituency, party, criminal_cases, education, total_assets, liabilities))
            records_count += 1
            if target_records is not None and records_count == target_records:
                break

async def main():
    db1_name = 'ap_data1.db'
    db2_name = 'ap_data2.db'

    conn_contestants, cursor1 = await setup_database(db1_name)
    conn_winners, cursor2 = await setup_database(db2_name)

    browser = await launch(headless=True, args=['--no-sandbox', '--disable-setuid-sandbox'])                   #configuration commonly used in headless browser setups
    contestants_url1 = 'https://myneta.info/andhrapradesh2019/index.php'
    params1 = 'action=summary&subAction=crime&sort=liabi'

    winners_url2 = 'https://www.myneta.info/andhrapradesh2019/index.php?action=show_winners&sort=candidate'

    # Scrape pages from contestants_url1 into ap_data1
    for i in range(1, 5):                                       #required data from 4 pages from 1, 4
        page_url = f"{ contestants_url1}?{params1}&page={i}"    #used to go to next page for scraping
        page = await browser.newPage()
        await page.goto(page_url)
        await scrape_data(page, cursor1)
        await page.close()

    # Scrape winners_url2 into ap_data2
    target_records = 174  #Since the web page has 174 records only we could specify it to scrape
    page_url = f"{winners_url2}"
    page = await browser.newPage()
    await page.goto(page_url)
    await scrape_data(page, cursor2, target_records)
    await page.close()

    await close_database(conn_contestants)
    await close_database(conn_winners)
    await browser.close()
    print("Data has been successfully stored in both SQLite databases.")

asyncio.run(main())              #runs main main first






    #DATA CLEANING

# Connect to the SQLite database
conn = sqlite3.connect('ap_data1.db')
conn = sqlite3.connect('ap_data2.db')
# Create a cursor object to execute SQL commands
cursor = conn.cursor()

# Execute the DELETE command
cursor.execute("DELETE FROM criminal_cases WHERE Sno IS NULL OR Sno = ''")
# Execute the DELETE command
cursor.execute("""
    DELETE FROM criminal_cases
    WHERE ROWID IN (
        SELECT ROWID
        FROM criminal_cases
        WHERE (Sno= 'Share On:' OR Sno = 'Download App')

    )
""")


# Commit the changes to the database
conn.commit()

# Close the cursor and connection
cursor.close()
conn.close()





print("\n")
print("* "*25+" DESCRIPTIVE ANALYSIS "+" *"*25)
print("\n")



#DESCRIPTIVE ANALYSIS
print("\n")
print("1: "+" Descriptive Analysis of Candidates and Winners in Political Data ")
print("\n")



# Connect to the SQLite databases
data_candidates = sqlite3.connect('ap_data1.db')
data_winners = sqlite3.connect('ap_data2.db')

# Read the data into pandas DataFrames
df_candidates = pd.read_sql_query("SELECT * FROM criminal_cases", data_candidates)
df_winners = pd.read_sql_query("SELECT * FROM criminal_cases", data_winners)

# Describe the candidates' data
print("Descriptive Analysis for Candidates:")
print(df_candidates.describe())

# Describe the winners' data
print("\nDescriptive Analysis for Winners:")
print(df_winners.describe())

# Close the database connections
data_candidates.close()
data_winners.close()




print("\n")
print("2 : "+"education qualification ")
print("\n")




# Connect to the SQLite databases
conn_contestants = sqlite3.connect('ap_data1.db')
conn_winners = sqlite3.connect('ap_data2.db')

# Query to fetch education qualification of contestants from ap_data1 [contestants]
query1 = "SELECT Education FROM criminal_cases"
df_contestants = pd.read_sql_query(query1, conn_contestants)

# Query to fetch education qualification of winners from ap_data2 [winner data]
query2 = "SELECT Education FROM criminal_cases"
df_winners = pd.read_sql_query(query2, conn_winners)

# Close database connections
conn_contestants.close()
conn_winners.close()

# Display education qualification of contestants
print("Education Qualification of Contestants:")
print(df_contestants['Education'].value_counts())

# Display education qualification of winners
print("\nEducation Qualification of Winners:")
print(df_winners['Education'].value_counts())




print("\n")
print("* "*25+" DATA VISUALIZATION "+" *"*25)
print("\n")



print("\n")
print("1 : "+" party wise total number of constituency ")
print("\n")




#party wise total number of constituency
def fetch_party_wise_constituency_counts(db_name, table_name):
    conn = sqlite3.connect(db_name)
    cursor = conn.cursor()
    cursor.execute(f'SELECT Party, COUNT(DISTINCT Constituency) FROM criminal_cases GROUP BY Party')
    party_counts = cursor.fetchall()
    conn.close()
    return party_counts

def visualize_party_wise_counts_pie(party_counts):
    parties = [party[0] for party in party_counts]
    counts = [party[1] for party in party_counts]

    plt.figure(figsize=(8, 8))
    colors = ['blue' if party == 'YSRCP' else 'yellow' if party == 'TDP' else 'red' for party in parties]
    plt.pie(counts, labels=parties, autopct='%1.1f%%', startangle=140, colors=colors)
    plt.title('Party-wise Total Number of Constituencies Won')
    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
    plt.show()

# Specify the database name and table name for winners
winners_db_name = 'ap_data2.db'
winners_table_name = 'winners'

# Fetch party-wise constituency counts from the winners database
party_counts = fetch_party_wise_constituency_counts(winners_db_name, winners_table_name)

# Visualize the party-wise counts using a pie chart with custom colors
visualize_party_wise_counts_pie(party_counts)








print("\n")
print("2 : "+"Party-wise Analysis of Candidates and Winners in Political Databases ")
print("\n")



# Connect to the SQLite databases
conn_contestants = sqlite3.connect('ap_data1.db')
conn_winners = sqlite3.connect('ap_data2.db')

# Assuming you have DataFrames df1 and df2 containing your data from databases 1 and 2

# Data Visualization for ap_data1  [contestants data]
df1 = pd.read_sql_query("SELECT * FROM criminal_cases", conn_contestants)
plt.figure(figsize=(9, 9))
sns.countplot(data=df1, x='Party', order=df1['Party'].value_counts().index)
plt.title('Number of Candidates per Party in  ap_data1 [contestants]')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

# Data Visualization for ap_data2 [winner data]
df2 = pd.read_sql_query("SELECT * FROM criminal_cases", conn_winners)
plt.figure(figsize=(8, 6))
sns.countplot(data=df2, x='Party', order=df2['Party'].value_counts().index)
plt.title('Number of Winners per Party in  ap_data2 [winner data]')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

# Close the database connections
conn_contestants.close()
conn_winners.close()



print("\n")
print("3 : "+" Educational Qualifications of Contestants and Winners in Political Databases ")
print("\n")



#bar G
# Connect to the SQLite databases
conn_contestants = sqlite3.connect('ap_data1.db')
conn_winners = sqlite3.connect('ap_data2.db')

# Query to fetch education qualification of contestants from ap_data1  [contestants data]
query1 = "SELECT Education FROM criminal_cases"
df_contestants = pd.read_sql_query(query1, conn_contestants)

# Query to fetch education qualification of winners from ap_data2 [winner data]
query2 = "SELECT Education FROM criminal_cases"
df_winners = pd.read_sql_query(query2, conn_winners)

# Close database connections
conn_contestants.close()
conn_winners.close()

# Set up the figure and axes for plotting
fig, axes = plt.subplots(1, 2, figsize=(12, 6))

# Plot for contestants
sns.countplot(data=df_contestants, x='Education', ax=axes[0], order=df_contestants['Education'].value_counts().index)
axes[0].set_title('Education Qualification of Contestants')
axes[0].set_xlabel('Education Level')
axes[0].set_ylabel('Count')
axes[0].tick_params(axis='x', rotation=90)  # Rotate x-axis labels for better readability

# Plot for winners
sns.countplot(data=df_winners, x='Education', ax=axes[1], order=df_winners['Education'].value_counts().index)
axes[1].set_title('Education Qualification of Winners')
axes[1].set_xlabel('Education Level')
axes[1].set_ylabel('Count')
axes[1].tick_params(axis='x', rotation=90)  # Rotate x-axis labels for better readability

# Adjust layout
plt.tight_layout()

# Show the plots
plt.show()



print("\n")
print("4 : "+" Distribution of Criminal Cases Among Contestants and Winners ")
print("\n")




# Connect to the SQLite databases
conn1 = sqlite3.connect('ap_data1.db')
conn2 = sqlite3.connect('ap_data2.db')

# Query to fetch criminal cases of contestants from database 1
query1 = "SELECT Criminal_Cases FROM criminal_cases"
df_contestants = pd.read_sql_query(query1, conn1)

# Query to fetch criminal cases of winners from database 2
query2 = "SELECT Criminal_Cases FROM criminal_cases"
df_winners = pd.read_sql_query(query2, conn2)

# Close database connections
conn1.close()
conn2.close()

# Count the occurrences of each criminal case for contestants and winners
contestants_counts = df_contestants['Criminal_Cases'].value_counts()
winners_counts = df_winners['Criminal_Cases'].value_counts()

# Convert Series to DataFrame for scatter plot
df_contestants_counts = pd.DataFrame({'Criminal_Cases': contestants_counts.index, 'Count': contestants_counts.values})
df_winners_counts = pd.DataFrame({'Criminal_Cases': winners_counts.index, 'Count': winners_counts.values})

# Create scatter plot for both contestants and winners
plt.scatter(df_contestants_counts['Criminal_Cases'], df_contestants_counts['Count'], color='skyblue', label='Contestants')
plt.scatter(df_winners_counts['Criminal_Cases'], df_winners_counts['Count'], color='lightgreen', label='Winners')

# Set plot title and labels
plt.title('Distribution of Criminal Cases Among Contestants and Winners')
plt.xlabel('Criminal Cases')
plt.ylabel('Count')

# Add legend
plt.legend()

# Show the scatter plot
plt.show()


print("\n")
print("5 : "+"Party-wise Combined Wealth of Contestants and Winners ")
print("\n")



# Connect to the SQLite databases
conn_contestants = sqlite3.connect('ap_data1.db')
conn_winners = sqlite3.connect('ap_data2.db')

# Query to fetch party-wise candidates' total assets and liabilities from database 1
query1 = "SELECT Party, Total_Assets, Liabilities FROM criminal_cases"
df_contestants = pd.read_sql_query(query1, conn_contestants)

# Query to fetch party-wise winners' total assets and liabilities from database 2
query2 = "SELECT Party, Total_Assets, Liabilities FROM criminal_cases"
df_winners = pd.read_sql_query(query2, conn_winners)

# Close database connections
conn_contestants.close()
conn_winners.close()

# Convert Total_Assets and Liabilities columns to numeric data type
df_contestants['Total_Assets'] = pd.to_numeric(df_contestants['Total_Assets'], errors='coerce')
df_contestants['Liabilities'] = pd.to_numeric(df_contestants['Liabilities'], errors='coerce')
df_winners['Total_Assets'] = pd.to_numeric(df_winners['Total_Assets'], errors='coerce')
df_winners['Liabilities'] = pd.to_numeric(df_winners['Liabilities'], errors='coerce')

# Calculate combined wealth (Total Assets - Liabilities) for contestants and winners
df_contestants['Combined_Wealth'] = df_contestants['Total_Assets'] - df_contestants['Liabilities']
df_winners['Combined_Wealth'] = df_winners['Total_Assets'] - df_winners['Liabilities']

# Group by party and calculate the sum of combined wealth for contestants and winners
contestants_party_wealth = df_contestants.groupby('Party')['Combined_Wealth'].sum()
winners_party_wealth = df_winners.groupby('Party')['Combined_Wealth'].sum()

# Create bar graphs for party-wise combined wealth
fig, axes = plt.subplots(1, 2, figsize=(12, 6))

# Bar graph for contestants' party-wise combined wealth
contestants_party_wealth.plot(kind='bar', ax=axes[0], color='skyblue')
axes[0].set_title('Party-wise Combined Wealth of Contestants')
axes[0].set_xlabel('Party')
axes[0].set_ylabel('Combined Wealth')

# Bar graph for winners' party-wise combined wealth
winners_party_wealth.plot(kind='bar', ax=axes[1], color='lightgreen')
axes[1].set_title('Party-wise Combined Wealth of Winners')
axes[1].set_xlabel('Party')
axes[1].set_ylabel('Combined Wealth')

# Adjust layout
plt.tight_layout()

# Show the bar graphs
plt.show()



print("\n")
print("* "*25+" REGRESSION ANALYSIS "+" *"*25)
print("\n")




#REGRESSION ANALYSIS
print("\n")
print("1: "+" linear regression analysis ")
print("\n")


#linear regression analysis
from sklearn.linear_model import LinearRegression
# Connect to the SQLite databases
conn_contestants = sqlite3.connect('ap_data1.db')
conn_winners = sqlite3.connect('ap_data2.db')

# Query to fetch relevant data for contestants and winners
query_contestants = "SELECT Criminal_Cases, Total_Assets FROM criminal_cases"
query_winners = "SELECT Criminal_Cases, Total_Assets FROM criminal_cases"

# Read data into DataFrames
df_contestants = pd.read_sql_query(query_contestants, conn_contestants)
df_winners = pd.read_sql_query(query_winners, conn_winners)

# Close database connections
conn_contestants.close()
conn_winners.close()

# Fill missing values with 0 and convert columns to numeric type
df_contestants['Total_Assets'] = pd.to_numeric(df_contestants['Total_Assets'].fillna(0), errors='coerce')
df_contestants['Criminal_Cases'] = pd.to_numeric(df_contestants['Criminal_Cases'].fillna(0), errors='coerce')

df_winners['Total_Assets'] = pd.to_numeric(df_winners['Total_Assets'].fillna(0), errors='coerce')
df_winners['Criminal_Cases'] = pd.to_numeric(df_winners['Criminal_Cases'].fillna(0), errors='coerce')

# Filter out rows with missing or non-numeric values
df_contestants = df_contestants.dropna(subset=['Total_Assets', 'Criminal_Cases'])
df_winners = df_winners.dropna(subset=['Total_Assets', 'Criminal_Cases'])

# Perform linear regression for Criminal Cases and Total Assets for contestants
X_contestants = df_contestants['Criminal_Cases'].values.reshape(-1, 1)
y_contestants = df_contestants['Total_Assets'].values.reshape(-1, 1)

reg_criminal_contestants = LinearRegression()
reg_criminal_contestants.fit(X_contestants, y_contestants)

# Perform linear regression for Criminal Cases and Total Assets for winners
X_winners = df_winners['Criminal_Cases'].values.reshape(-1, 1)
y_winners = df_winners['Total_Assets'].values.reshape(-1, 1)

reg_criminal_winners = LinearRegression()
reg_criminal_winners.fit(X_winners, y_winners)

# Print the coefficients of the best-fit lines
print('Contestants - Criminal Cases vs. Total Assets:')
print('Slope:', reg_criminal_contestants.coef_[0][0])
print('Intercept:', reg_criminal_contestants.intercept_[0])

print('\nWinners - Criminal Cases vs. Total Assets:')
print('Slope:', reg_criminal_winners.coef_[0][0])
print('Intercept:', reg_criminal_winners.intercept_[0])


print("\n")
print("2: "+"Multiple Linear Regression ")
print("\n")


# Connect to the SQLite databases
conn_contestants = sqlite3.connect('ap_data1.db')
conn_winners = sqlite3.connect('ap_data2.db')

# Query to fetch relevant data for contestants and winners
query_contestants = "SELECT Criminal_Cases, Total_Assets FROM criminal_cases"
query_winners = "SELECT Criminal_Cases, Total_Assets FROM criminal_cases"

# Read data into DataFrames
df_contestants = pd.read_sql_query(query_contestants, conn_contestants)
df_winners = pd.read_sql_query(query_winners, conn_winners)

# Close database connections
conn_contestants.close()
conn_winners.close()

# Fill missing values with 0 and convert columns to numeric type
df_contestants['Total_Assets'] = pd.to_numeric(df_contestants['Total_Assets'].fillna(0), errors='coerce')
df_contestants['Criminal_Cases'] = pd.to_numeric(df_contestants['Criminal_Cases'].fillna(0), errors='coerce')

df_winners['Total_Assets'] = pd.to_numeric(df_winners['Total_Assets'].fillna(0), errors='coerce')
df_winners['Criminal_Cases'] = pd.to_numeric(df_winners['Criminal_Cases'].fillna(0), errors='coerce')

# Filter out rows with missing or non-numeric values
df_contestants = df_contestants.dropna(subset=['Total_Assets', 'Criminal_Cases'])
df_winners = df_winners.dropna(subset=['Total_Assets', 'Criminal_Cases'])

# Add a constant column to the DataFrames for the intercept term
df_contestants['Intercept'] = 1
df_winners['Intercept'] = 1

# Perform multiple linear regression for contestants
X_contestants = df_contestants[['Criminal_Cases', 'Intercept']]
y_contestants = df_contestants['Total_Assets']

model_contestants = sm.OLS(y_contestants, X_contestants)
results_contestants = model_contestants.fit()

# Perform multiple linear regression for winners
X_winners = df_winners[['Criminal_Cases', 'Intercept']]
y_winners = df_winners['Total_Assets']

model_winners = sm.OLS(y_winners, X_winners)
results_winners = model_winners.fit()

# Print the summary of regression results
print('Contestants - Regression Summary:')
print(results_contestants.summary())

print('\nWinners - Regression Summary:')
print(results_winners.summary())



print("\n")
print("* "*25+" CORRELATION ANALYSIS "+" *"*25)
print("\n")



print("\n")
print("1: "+" Correlation between Criminal Cases and Total Assets in Election Candidates and Winners ")
print("\n")

#CORRELATION ANALYSIS
# Correlation between Criminal Cases and Total Assets in Election Candidates and Winners
from scipy.stats import pearsonr

# Load data from SQLite databases
def load_data(db_name, table_name):
    conn = sqlite3.connect(db_name)
    query = f"SELECT Criminal_Cases, Total_Assets FROM {table_name}"
    df = pd.read_sql_query(query, conn)
    conn.close()
    # Convert data types to numeric if needed
    df['Criminal_Cases'] = pd.to_numeric(df['Criminal_Cases'], errors='coerce')
    df['Total_Assets'] = pd.to_numeric(df['Total_Assets'], errors='coerce')
    return df.dropna()

# Calculate Pearson correlation coefficient and p-value
def calculate_correlation(df):
    correlation_coefficient, p_value = pearsonr(df['Criminal_Cases'], df['Total_Assets'])
    return correlation_coefficient, p_value

# Hypothesis Testing
def hypothesis_test(p_value, alpha=0.05):
    if p_value < alpha:
        return "Reject Null Hypothesis: There is a significant relationship."
    else:
        return "Fail to Reject Null Hypothesis: Relationship may not be significant."

# Load data for candidates and winners
candidates_df = load_data('ap_data1.db', 'criminal_cases')
winners_df = load_data('ap_data2.db', 'criminal_cases')

# Calculate correlation for candidates and winners
candidates_corr, candidates_p_value = calculate_correlation(candidates_df)
winners_corr, winners_p_value = calculate_correlation(winners_df)

# Perform hypothesis testing
candidates_test_result = hypothesis_test(candidates_p_value)
winners_test_result = hypothesis_test(winners_p_value)

# Print results
print("Candidates Correlation Coefficient:", candidates_corr)
print("Candidates Hypothesis Test Result:", candidates_test_result)
print("Winners Correlation Coefficient:", winners_corr)
print("Winners Hypothesis Test Result:", winners_test_result)



print("\n")
print("2: "+"Correlation between Criminal Cases and liabilities in Election Candidates and Winners ")
print("\n")
#CORRELATION ANALYSIS
# Correlation between Criminal Cases and liabilities in Election Candidates and Winners
# Load data from SQLite databases
def load_data(db_name, table_name):
    conn = sqlite3.connect(db_name)
    query = f"SELECT Criminal_Cases, Liabilities FROM {table_name}"
    df = pd.read_sql_query(query, conn)
    conn.close()
    # Convert data types to numeric if needed
    df['Criminal_Cases'] = pd.to_numeric(df['Criminal_Cases'], errors='coerce')
    df['Liabilities'] = pd.to_numeric(df['Liabilities'], errors='coerce')
    return df.dropna()

# Calculate Pearson correlation coefficient and p-value
def calculate_correlation(df):
    correlation_coefficient, p_value = pearsonr(df['Criminal_Cases'], df['Liabilities'])
    return correlation_coefficient, p_value

# Hypothesis Testing
def hypothesis_test(p_value, alpha=0.05):
    if p_value < alpha:
        return "Reject Null Hypothesis: There is a significant relationship."
    else:
        return "Fail to Reject Null Hypothesis: Relationship may not be significant."

# Load data for candidates and winners
candidates_df = load_data('ap_data1.db', 'criminal_cases')
winners_df = load_data('ap_data2.db', 'criminal_cases')

# Calculate correlation for candidates and winners
candidates_corr, candidates_p_value = calculate_correlation(candidates_df)
winners_corr, winners_p_value = calculate_correlation(winners_df)

# Perform hypothesis testing
candidates_test_result = hypothesis_test(candidates_p_value)
winners_test_result = hypothesis_test(winners_p_value)

# Print results
print("Candidates Correlation Coefficient:", candidates_corr)
print("Candidates Hypothesis Test Result:", candidates_test_result)
print("Winners Correlation Coefficient:", winners_corr)
print("Winners Hypothesis Test Result:", winners_test_result)